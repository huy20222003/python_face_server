import numpy as np
import logging
import os
import tensorflow as tf
import cv2
from typing import Optional
import mediapipe as mp

class FaceRecognitionSystem:
    def __init__(self, model_path: str = "models/arcface_model.tflite", threshold: float = 0.5):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng m√¥ h√¨nh TF Lite.
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n l∆∞u m√¥ h√¨nh TF Lite.
            threshold: Ng∆∞·ª°ng nh·∫≠n di·ªán.
        """
        self.threshold = threshold
        self.model_path = model_path
        self.model = None  # B·ªô nh·ªõ ƒë·ªám cho m√¥ h√¨nh TF Lite
        self.interpreter = None
        self.input_details = None
        self.output_details = None
        self._setup_logging()
        self._load_model()
        self.face_detector = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)
        
    def _detect_faces(self, image: np.ndarray) -> list:
        """
        Ph√°t hi·ªán t·∫•t c·∫£ c√°c khu√¥n m·∫∑t trong ·∫£nh v√† tr·∫£ v·ªÅ danh s√°ch c√°c v√πng ch·ª©a khu√¥n m·∫∑t.
        M·ªói khu√¥n m·∫∑t ƒë∆∞·ª£c c·∫Øt ra d∆∞·ªõi d·∫°ng m·ªôt numpy array.
        """
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_detector.process(image_rgb)
        faces = []
        
        if results.detections:
            h, w, _ = image.shape
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box
                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)
                # ƒê·∫£m b·∫£o t·ªça ƒë·ªô kh√¥ng v∆∞·ª£t ngo√†i bi√™n ·∫£nh
                x, y = max(0, x), max(0, y)
                face_img = image[y:y + height, x:x + width]
                faces.append(face_img)
        return faces

    def _setup_logging(self) -> None:
        """C·∫•u h√¨nh logging."""
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    def _load_model(self) -> None:
        """T·∫£i m√¥ h√¨nh TF Lite v√†o b·ªô nh·ªõ n·∫øu ch∆∞a ƒë∆∞·ª£c t·∫£i."""
        if self.model is None:
            if not os.path.exists(self.model_path):
                self.logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh t·∫°i {self.model_path}.")
                raise FileNotFoundError("M√¥ h√¨nh kh√¥ng t·ªìn t·∫°i!")
            try:
                self.logger.info("üîÑ ƒêang t·∫£i m√¥ h√¨nh TensorFlow Lite v√†o b·ªô nh·ªõ...")
                self.interpreter = tf.lite.Interpreter(model_path=self.model_path)
                self.interpreter.allocate_tensors()
                self.input_details = self.interpreter.get_input_details()
                self.output_details = self.interpreter.get_output_details()
                self.logger.info("‚úÖ M√¥ h√¨nh TensorFlow Lite ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
            except Exception as e:
                self.logger.error(f"‚ùå L·ªói t·∫£i m√¥ h√¨nh: {e}")
                raise

    def _preprocess_image(self, image: np.ndarray) -> Optional[np.ndarray]:
        """
        Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh.
        """
        try:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.shape[2] == 3 else image
            input_size = self.input_details[0]['shape'][1:3]  # L·∫•y k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o t·ª´ m√¥ h√¨nh
            image = cv2.resize(image, tuple(input_size))
            image = image.astype(np.float32) / 255.0  # Chu·∫©n h√≥a v·ªÅ kho·∫£ng [0,1]
            return np.expand_dims(image, axis=0)
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
            return None

    def get_embeddings(self, image: np.ndarray) -> Optional[list]:
        """
        Tr√≠ch xu·∫•t embedding t·ª´ t·∫•t c·∫£ c√°c khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán trong ·∫£nh.
        Tr·∫£ v·ªÅ danh s√°ch c√°c embedding ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a.
        N·∫øu kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t n√†o, tr·∫£ v·ªÅ None.
        """
        face_images = self._detect_faces(image)
        if not face_images:
            self.logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o trong ·∫£nh.")
            return None

        embeddings = []
        for face_image in face_images:
            preprocessed_image = self._preprocess_image(face_image)
            if preprocessed_image is None:
                continue
            try:
                self.interpreter.set_tensor(self.input_details[0]['index'], preprocessed_image)
                self.interpreter.invoke()
                embedding = self.interpreter.get_tensor(self.output_details[0]['index'])[0]
                # Chu·∫©n h√≥a embedding v√† th√™m v√†o danh s√°ch
                normalized_embedding = embedding / np.linalg.norm(embedding)
                embeddings.append(normalized_embedding)
            except Exception as e:
                self.logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t embedding: {e}")
                continue

        if not embeddings:
            return None
        return embeddings

    def compare_faces(self, embedding1: Optional[np.ndarray], embedding2: Optional[np.ndarray]) -> bool:
        """So s√°nh hai embeddings ƒë·ªÉ x√°c th·ª±c khu√¥n m·∫∑t."""
        try:
            if embedding1 is None or embedding2 is None:
                return False
            similarity = np.dot(embedding1, embedding2)
            return similarity > self.threshold
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói so s√°nh khu√¥n m·∫∑t: {e}")
            return False